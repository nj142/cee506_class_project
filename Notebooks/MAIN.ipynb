{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------ #\n",
    "#                  IMPORT FUNCTIONS                #\n",
    "# ------------------------------------------------ #\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "# Get the project root directory (going up one level from Notebooks)\n",
    "project_root = os.path.dirname(os.getcwd())\n",
    "# Add the Scripts directory to Python path so we can import modules from individual scripts files\n",
    "scripts_dir = os.path.join(project_root, 'Scripts')\n",
    "if scripts_dir not in sys.path:\n",
    "    sys.path.append(scripts_dir)\n",
    "\n",
    "# Add functions here from Scripts folder\n",
    "from ao_indices import return_AO_fetch\n",
    "from enso_indices import return_ENSO_fetch\n",
    "from breakup_data import calculate_breakup_data\n",
    "from breakup_data import calculate_estimated_breakup\n",
    "from bounding_box import calculate_bounding_box\n",
    "from raster_temp import mean_30d_raster_temps\n",
    "#from raster_temp_era5 import mean_30d_raster_temps\n",
    "from correlation_raster import calculate_ao_correlation, calculate_enso_correlation, save_results_to_csv, plot_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------ #\n",
    "#                CLASS DEFINITION                  #\n",
    "# ------------------------------------------------ #\n",
    "\n",
    "class StudySite:\n",
    "    def __init__(self,name):\n",
    "        self.name = name\n",
    "        self.breakup_anomaly_data = None # Dictionary of integer years (e.g. 2000) with following data subkeys: zscore_index, anomaly_days, breakup_date, breakup_doy\n",
    "        self.estimated_breakup_doy = None # The MEAN breakup date for the study site across all years, in DOY integer format.\n",
    "        self.enso_indices = None # Dictionary of integer years (e.g. 2000) with enso_value for each of 12 months before breakup, named as strings\n",
    "        self.ao_indices = None # Dictionary of integer years (e.g. 2000) with ao_value for each of 12 months before breakup\n",
    "        self.bounding_box = None  # Dictionary of max_lat:value, min_lat:value, max_lon:value, min_lon:value\n",
    "        self.mean_30d_raster_temps = None # Dictionary of integer years (e.g. 2000) with rasters of the mean breakup temperature across the 30 days prior to mean breakup date (we can't do actual breakup date because months have different temps)\n",
    "        self.enso_correlations = None # Single raster of ENSO correlations across all years\n",
    "        self.ao_correlations = None # Single raster of AO correlations across all years\n",
    "\n",
    "\n",
    "    def calculate_breakup_data(self, ice_data_filepath, years_of_data):\n",
    "        \"\"\" Load ice breakup data for this site from the provided icedata spreadsheet, filtering by years_of_data and the site name \"\"\"\n",
    "\n",
    "        # Dictionary of integer years (e.g. 2000) with following data subkeys: zscore_index, anomaly_days, breakup_doy\n",
    "        self.breakup_anomaly_data = calculate_breakup_data(years_of_data, ice_data_filepath, self.name)\n",
    "\n",
    "        # Single value of estimated breakup day of year for this site (MEAN across all years)\n",
    "        self.estimated_breakup_doy = calculate_estimated_breakup(self.breakup_anomaly_data)\n",
    "\n",
    "\n",
    "    def fetch_enso(self,enso_data_filepath):\n",
    "        \"\"\" Pull in ENSO data as dictionary of integer years (e.g. 2000) on a MONTHLY scale for 12 months prior to estimated breakup \n",
    "            date from all years of data (excluding month with breakup date, as we are looking for time lag of 1+ months) \"\"\"\n",
    "        \n",
    "        # Dictionary of integer years (e.g. 2000) with dictionary of preceding 12 month_name: month_values\n",
    "        self.enso_indices = return_ENSO_fetch(enso_data_filepath, self.breakup_anomaly_data, self.estimated_breakup_doy)\n",
    "\n",
    "\n",
    "    def fetch_ao(self,ao_data_filepath):\n",
    "        \"\"\" Pull in AO data as dictionary of integer years (e.g. 2000) on a MONTHLY scale for 12 months prior to estimated breakup \n",
    "            date from all years of data (excluding month with breakup date, as we are looking for time lag of 1+ months) \"\"\"\n",
    "        \n",
    "        # Dictionary of integer years (e.g. 2000) with dictionary of preceding 12 month_name: month_values\n",
    "        self.ao_indices = return_AO_fetch(ao_data_filepath, self.breakup_anomaly_data, self.estimated_breakup_doy)\n",
    "\n",
    "\n",
    "    def calculate_bounding_box(self, ice_data_filepath):\n",
    "        \"\"\" Calculate 100km bounding box based on study site coordinates \"\"\"\n",
    "\n",
    "        # Bounding box for this site based on lat,lon for each site in the ice data CSV\n",
    "        # Dictionary of max_lat:value, min_lat:value, max_lon:value, min_lon:value\n",
    "        self.bounding_box = calculate_bounding_box(ice_data_filepath, self.name)\n",
    "\n",
    "\n",
    "    def calculate_mean_raster_temps(self):\n",
    "        \"\"\" Pull the raster data within the bounding box for each of the 30d prior to the given breakup date variable, then average all the rasters together to return the mean 30d temp for each year.\"\"\"\n",
    "\n",
    "        # Dictionary of integer years (e.g. 2000) paired with rasters of the mean breakup temperature across the 30 days prior to given breakup date variable for each year\n",
    "        # (Note we can't do actual breakup date for each year for each site due to statistical reasons, so for each site we will find the mean breakup date and use that to find our 30d period)\n",
    "        self.mean_30d_raster_temps = mean_30d_raster_temps(self.bounding_box, self.breakup_anomaly_data, self.estimated_breakup_doy)\n",
    "       \n",
    "    # For each month in our indices in the 12 months prior to breakup (excluding breakup month,) calculate the P value across all years of data\n",
    "    # Single raster for each of these consisting of correlation across all years\n",
    "    def calculate_ao_correlation(self, ice_data_filepath, ao_data_filepath, project_root, lag=1):\n",
    "        \"\"\"Calculate AO correlations for this study site.\"\"\"\n",
    "        self.ao_correlations = calculate_ao_correlation(\n",
    "            site_name=self.name,\n",
    "            ice_data_filepath=ice_data_filepath,\n",
    "            ao_data_filepath=ao_data_filepath,\n",
    "            project_root=project_root,\n",
    "            lag=lag\n",
    "        )\n",
    "\n",
    "    def calculate_enso_correlation(self, ice_data_filepath, enso_data_filepath, project_root, lag=1):\n",
    "        \"\"\"Calculate ENSO correlations for this study site.\"\"\"\n",
    "        self.enso_correlations = calculate_enso_correlation(\n",
    "            site_name=self.name,\n",
    "            ice_data_filepath=ice_data_filepath,\n",
    "            enso_data_filepath=enso_data_filepath,\n",
    "            project_root=project_root,\n",
    "            lag=lag\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------ #\n",
    "#                  PARAMETERS                      #\n",
    "# ------------------------------------------------ #\n",
    "\n",
    "# Add data here from Data folder\n",
    "enso_data_filepath = os.path.join(project_root, \"Data\", \"ENSO_index.txt\")\n",
    "ao_data_filepath = os.path.join(project_root, \"Data\", \"AO_index.txt\")\n",
    "ice_data_filepath = os.path.join(project_root, \"Data\", \"ice_data.csv\")\n",
    "results_dir = os.path.join(project_root, \"Results\")\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Choose study sites here\n",
    "study_site_names = [\"Stebbins\"]\n",
    "\n",
    "# Generate a list of years from 2000 to 2022 - select years\n",
    "years_of_data = list(range(2000, 2023))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Noah\\Desktop\\PhD\\Coding\\ce506_class_project\\cee506_class_project\\Scripts\\ao_indices.py:22: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  df = pd.read_csv(ao_data_filepath, delim_whitespace=True, index_col=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing AO data: Error tokenizing data. C error: Expected 13 fields in line 52, saw 14\n",
      "\n",
      "Results saved to c:\\Users\\Noah\\Desktop\\PhD\\Coding\\ce506_class_project\\cee506_class_project\\Results\\ao_correlations_Stebbins.csv\n",
      "AO correlations saved for Stebbins\n",
      "Results saved to c:\\Users\\Noah\\Desktop\\PhD\\Coding\\ce506_class_project\\cee506_class_project\\Results\\enso_correlations_Stebbins.csv\n",
      "ENSO correlations saved for Stebbins\n",
      "Finished processing site: Stebbins\n",
      "{2000: {'breakup_date': datetime.datetime(2000, 6, 8, 12, 0), 'zscore_index': np.float64(13.543478260869565), 'anomaly_days': np.float64(1.0), 'breakup_doy': np.float64(160.5)}, 2001: {'breakup_date': datetime.datetime(2001, 6, 8, 0, 0), 'zscore_index': np.float64(12.043478260869565), 'anomaly_days': np.float64(2.0), 'breakup_doy': np.float64(159.0)}, 2002: {'breakup_date': datetime.datetime(2002, 5, 27, 12, 0), 'zscore_index': np.float64(0.5434782608695627), 'anomaly_days': np.float64(1.0), 'breakup_doy': np.float64(147.5)}, 2003: {'breakup_date': datetime.datetime(2003, 5, 25, 12, 0), 'zscore_index': np.float64(-1.4565217391304373), 'anomaly_days': np.float64(3.0), 'breakup_doy': np.float64(145.5)}, 2004: {'breakup_date': datetime.datetime(2004, 5, 11, 0, 0), 'zscore_index': np.float64(-14.956521739130435), 'anomaly_days': np.float64(14.0), 'breakup_doy': np.float64(132.0)}, 2005: {'breakup_date': datetime.datetime(2005, 5, 26, 12, 0), 'zscore_index': np.float64(-0.4565217391304372), 'anomaly_days': np.float64(3.0), 'breakup_doy': np.float64(146.5)}, 2006: {'breakup_date': datetime.datetime(2006, 6, 3, 12, 0), 'zscore_index': np.float64(7.543478260869563), 'anomaly_days': np.float64(1.0), 'breakup_doy': np.float64(154.5)}, 2007: {'breakup_date': datetime.datetime(2007, 6, 1, 12, 0), 'zscore_index': np.float64(5.543478260869563), 'anomaly_days': np.float64(1.0), 'breakup_doy': np.float64(152.5)}, 2008: {'breakup_date': datetime.datetime(2008, 5, 30, 12, 0), 'zscore_index': np.float64(4.543478260869563), 'anomaly_days': np.float64(1.0), 'breakup_doy': np.float64(151.5)}, 2009: {'breakup_date': datetime.datetime(2009, 6, 6, 0, 0), 'zscore_index': np.float64(10.043478260869565), 'anomaly_days': np.float64(4.0), 'breakup_doy': np.float64(157.0)}, 2010: {'breakup_date': datetime.datetime(2010, 6, 13, 0, 0), 'zscore_index': np.float64(17.043478260869563), 'anomaly_days': np.float64(12.0), 'breakup_doy': np.float64(164.0)}, 2011: {'breakup_date': datetime.datetime(2011, 6, 1, 0, 0), 'zscore_index': np.float64(5.043478260869563), 'anomaly_days': np.float64(2.0), 'breakup_doy': np.float64(152.0)}, 2012: {'breakup_date': datetime.datetime(2012, 6, 12, 0, 0), 'zscore_index': np.float64(17.043478260869563), 'anomaly_days': np.float64(4.0), 'breakup_doy': np.float64(164.0)}, 2013: {'breakup_date': datetime.datetime(2013, 6, 9, 0, 0), 'zscore_index': np.float64(13.043478260869565), 'anomaly_days': np.float64(6.0), 'breakup_doy': np.float64(160.0)}, 2014: {'breakup_date': datetime.datetime(2014, 5, 14, 12, 0), 'zscore_index': np.float64(-12.456521739130435), 'anomaly_days': np.float64(9.0), 'breakup_doy': np.float64(134.5)}, 2015: {'breakup_date': datetime.datetime(2015, 5, 15, 12, 0), 'zscore_index': np.float64(-11.456521739130435), 'anomaly_days': np.float64(13.0), 'breakup_doy': np.float64(135.5)}, 2016: {'breakup_date': datetime.datetime(2016, 5, 10, 0, 0), 'zscore_index': np.float64(-15.956521739130435), 'anomaly_days': np.float64(6.0), 'breakup_doy': np.float64(131.0)}, 2017: {'breakup_date': datetime.datetime(2017, 5, 19, 12, 0), 'zscore_index': np.float64(-7.456521739130437), 'anomaly_days': np.float64(1.0), 'breakup_doy': np.float64(139.5)}, 2018: {'breakup_date': datetime.datetime(2018, 5, 9, 12, 0), 'zscore_index': np.float64(-17.456521739130437), 'anomaly_days': np.float64(13.0), 'breakup_doy': np.float64(129.5)}, 2019: {'breakup_date': datetime.datetime(2019, 5, 10, 0, 0), 'zscore_index': np.float64(-16.956521739130437), 'anomaly_days': np.float64(6.0), 'breakup_doy': np.float64(130.0)}, 2020: {'breakup_date': datetime.datetime(2020, 5, 26, 12, 0), 'zscore_index': np.float64(0.5434782608695627), 'anomaly_days': np.float64(3.0), 'breakup_doy': np.float64(147.5)}, 2021: {'breakup_date': datetime.datetime(2021, 5, 27, 12, 0), 'zscore_index': np.float64(0.5434782608695627), 'anomaly_days': np.float64(1.0), 'breakup_doy': np.float64(147.5)}, 2022: {'breakup_date': datetime.datetime(2022, 5, 18, 12, 0), 'zscore_index': np.float64(-8.456521739130437), 'anomaly_days': np.float64(1.0), 'breakup_doy': np.float64(138.5)}}\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------ #\n",
    "#                  MAIN CODE                      #\n",
    "# ------------------------------------------------ #\n",
    "\n",
    "# Create list of StudySite objects\n",
    "sites = [StudySite(name) for name in study_site_names]\n",
    "\n",
    "# Process data for each site\n",
    "for site in sites:\n",
    "    # Calculate basic site data\n",
    "    site.calculate_breakup_data(ice_data_filepath, years_of_data)\n",
    "    \n",
    "    site.fetch_enso(enso_data_filepath)\n",
    "        \n",
    "    site.fetch_ao(ao_data_filepath)\n",
    "      \n",
    "    site.calculate_bounding_box(ice_data_filepath)\n",
    "    \n",
    "    site.calculate_mean_raster_temps()\n",
    "    \n",
    "    site.calculate_ao_correlation(\n",
    "        ice_data_filepath=ice_data_filepath,\n",
    "        ao_data_filepath=ao_data_filepath,\n",
    "        project_root=project_root,\n",
    "        lag=1\n",
    "    )\n",
    "    site.calculate_enso_correlation(\n",
    "        ice_data_filepath=ice_data_filepath,\n",
    "        enso_data_filepath=enso_data_filepath,\n",
    "        project_root=project_root,\n",
    "        lag=1\n",
    "    )\n",
    "    \n",
    "    print(f\"Finished processing site: {site.name}\")\n",
    "   \n",
    "    print(site.breakup_anomaly_data) # Dictionary of integer years (e.g. 2000) with enso_value for each of 12 months before breakup\n",
    "    \"\"\" self.enso_correlations = None # Single raster of ENSO correlations across all years\n",
    "        self.ao_correlations = None # Single raster of AO correlations across all years\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to c:\\Users\\Noah\\Desktop\\PhD\\Coding\\ce506_class_project\\cee506_class_project\\Results\\ao_correlation_Stebbins.csv\n",
      "Plots saved to c:\\Users\\Noah\\Desktop\\PhD\\Coding\\ce506_class_project\\cee506_class_project\\Results\\AO_Plots\n",
      "Results saved to c:\\Users\\Noah\\Desktop\\PhD\\Coding\\ce506_class_project\\cee506_class_project\\Results\\enso_correlation_Stebbins.csv\n",
      "Plots saved to c:\\Users\\Noah\\Desktop\\PhD\\Coding\\ce506_class_project\\cee506_class_project\\Results\\ENSO_Plots\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------ #\n",
    "#       CORRELATION AND PLOTTING (Optional)        #\n",
    "# ------------------------------------------------ #\n",
    "\n",
    "# Save and plot AO and ENSO correlations for each site\n",
    "for site in sites:\n",
    "    if site.ao_correlations is not None:\n",
    "        # Save AO correlations to CSV\n",
    "        ao_csv_path = os.path.join(results_dir, f\"ao_correlation_{site.name}.csv\")\n",
    "        save_results_to_csv(site.ao_correlations, ao_csv_path)\n",
    "        \n",
    "        # Plot AO correlations\n",
    "        ao_plot_dir = os.path.join(results_dir, \"AO_Plots\")\n",
    "        plot_results(site.ao_correlations, \"AO\", ao_plot_dir)\n",
    "    \n",
    "    if site.enso_correlations is not None:\n",
    "        # Save ENSO correlations to CSV\n",
    "        enso_csv_path = os.path.join(results_dir, f\"enso_correlation_{site.name}.csv\")\n",
    "        save_results_to_csv(site.enso_correlations, enso_csv_path)\n",
    "        \n",
    "        # Plot ENSO correlations\n",
    "        enso_plot_dir = os.path.join(results_dir, \"ENSO_Plots\")\n",
    "        plot_results(site.enso_correlations, \"ENSO\", enso_plot_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
